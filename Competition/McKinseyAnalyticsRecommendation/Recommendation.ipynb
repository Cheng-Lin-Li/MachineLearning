{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "## Problem Statement\n",
    "\n",
    "Your client is a fast-growing mobile platform, for hosting coding challenges. They have a unique business model, where they crowdsource problems from various creators(authors). These authors create the problem and release it on the client's platform. The users then select the challenges they want to solve. The authors make money based on the level of difficulty of their problems and how many users take up their challenge.\n",
    " \n",
    "The client, on the other hand makes money when the users can find challenges of their interest and continue to stay on the platform. Till date, the client has relied on its domain expertise, user interface and experience with user behaviour to suggest the problems a user might be interested in. You have now been appointed as the data scientist who needs to come up with the algorithm to keep the users engaged on the platform.\n",
    "The client has provided you with history of last 10 challenges the user has solved, and you need to predict which might be the next 3 challenges the user might be interested to solve. Apply your data science skills to help the client make a big mark in their user engagements/revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET DESCRIPTION\n",
    "### We have three data files:\n",
    "##### train.csv: It contains the set of 13 challenges that were attempted by the same user in a sequence \n",
    "\n",
    "|Variable | Definition|\n",
    "|------------- |-------------|\n",
    "|user_sequence|Unique ID for the sequence|\n",
    "|user_id|User ID|\n",
    "|challenge_sequence|Challenge sequence number (1-13)|\n",
    "|challenge|Challenge ID|\n",
    "\n",
    "##### challenge_data.csv: Contains attributes related to each challenge\n",
    "|Variable|Definition|\n",
    "|------------- |-------------|\n",
    "|challenge_ID|Challenge ID|\n",
    "|programming_language|Programming language for the challenge|\n",
    "|challenge_series_ID|Series for the given challenge|\n",
    "|total_submissions|Total submissions by all users|\n",
    "|publish_date|Publishing date for the challenge|\n",
    "|author_ID|Author ID|\n",
    "|author_gender|Author gender|\n",
    "|author_org_ID|Organization ID for author|\n",
    "|category_id|Type of challenge|\n",
    "\n",
    "##### test.csv:  \n",
    "Contains the first 10 challenges solved by a new user set (not in train) in the test set. We need to predict the next 3 sequence of challenges for these users.\n",
    "\n",
    "|Variable|Definition|\n",
    "|------------- |-------------|\n",
    "|user_sequence|Unique ID for the sequence|\n",
    "|user_id|User ID|\n",
    "|challenge_sequence|Challenge sequence number (1-10)|\n",
    "|challenge|Challenge ID|\n",
    "\n",
    "##### sample_submission.csv:  \n",
    "It contains the format for submission. Only submissions in this format are acceptable. This should have the next 3 challenges for each user.\n",
    "\n",
    "|Variable|Definition|\n",
    "|------------- |-------------|\n",
    "|user_sequence|Unique ID for the sequence (See Note)|\n",
    "|challenge|Challenge ID|\n",
    "\n",
    "###### Note: The format is given by \"user_id_challenge_sequence\". For example, for user ID 2 you must predict the next 3 challenges with sequence 11, 12 and 13 respectively. The corresponding user_sequence would be given by 2_11, 2_12 & 2_13.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from surprise import Reader, Dataset, SVD, SVDpp, evaluate\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge = pd.read_csv('challenge_data-revised.csv')\n",
    "# df_challenge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('train.csv')\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('test.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Recommender\n",
    "\n",
    "Computes similarity between challenges based on certain metrics and suggests challenges that are most similar to a particular challenge that a user liked. the challenge metadata (or content) will be used to build this engine, this also known as Content Based Filtering.\n",
    "\n",
    "Two Content Based Recommenders based on:\n",
    "\n",
    "    programming_language, author id, category id, total_submissions\n",
    "    programming_language, author id, author gender, author org, category id, total_submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge['publish_date'] = pd.to_datetime(df_challenge['publish_date'])\n",
    "# df_challenge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "### total_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"total_submissions\"].dropna(),\n",
    "#                  bins=20,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"total_submissions\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('total_submissions=> mean =%f, median=%f'%(df_challenge[\"total_submissions\"].mean(), df_challenge[\"total_submissions\"].median()))\n",
    "# df_challenge[\"total_submissions\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"total_submissions\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning for total submissions.\n",
    "The latest challenges do not have the number, use median number to fillin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assign missing total_submissions to median number\n",
    "# df_challenge[\"total_submissions\"] = df_challenge[\"total_submissions\"].fillna(value=122)\n",
    "# df_challenge.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge series id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"challenge_series_ID\"].dropna(),\n",
    "#                  bins=20,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"challenge_series_ID\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"challenge_series_ID\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use mode number of series_id for missing data\n",
    "Manually add the series id by previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_challenge[\"challenge_series_ID\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"challenge_series_ID\"] = df_challenge[\"challenge_series_ID\"].fillna(value='SI2652')\n",
    "# df_challenge.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"author_ID\"].dropna(),\n",
    "#                  bins=100,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"author_ID\")\n",
    "#     plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_challenge[\"author_ID\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"author_ID\"] = df_challenge[\"author_ID\"].fillna(value='AI565468')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"author_gender\"].dropna(),\n",
    "#                  bins=2,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"author_gender\")\n",
    "#     plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"author_gender\"] = df_challenge[\"author_gender\"].fillna(value='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"author_org_ID\"].dropna(),\n",
    "#                  bins=100,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"author_org_ID\")\n",
    "#     plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"author_org_ID\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"author_org_ID\"] = df_challenge[\"author_org_ID\"].fillna(value='AOI100201')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"category_id\"].dropna(),\n",
    "#                  bins=100,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"category_id\")\n",
    "#     plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"category_id\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge[\"category_id\"] = df_challenge[\"category_id\"].fillna(value=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sns.plotting_context(\"notebook\",font_scale=1.5):\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "#     sns.distplot(df_challenge[\"programming_language\"].dropna(),\n",
    "#                  bins=20,\n",
    "#                  kde=False,\n",
    "#                  color=\"green\")\n",
    "#     plt.title(\"programming_language\")\n",
    "#     plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_challenge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Based Recommender\n",
    "\n",
    "To build the standard metadata based content recommender, merging current dataset with the crew and the keyword datasets. \n",
    "\n",
    "Data preparation as first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Due to the lack of data content and distribution, the above based engine suffers from some severe limitations. \n",
    "It is only capable of suggesting challenges which are close to a certain challenge. \n",
    "That is, it is not capable of capturing tastes and providing recommendations across genres.\n",
    "Also, the engine is not really personal in that it doesn't capture the personal tastes and biases of a user. \n",
    "Anyone querying the engine for recommendations based on a challenge will receive the same recommendations for tmovie, regardless of who s/he is.\n",
    "\n",
    "Therefore, in this section, a technique called Collaborative Filtering will be used to make recommendations to users. \n",
    "Collaborative Filtering is based on the idea that users similar to a me can be used to predict how much I will like a particular product or service those users have used/experienced but I have not.\n",
    "The Surprise library provides algorithms like Singular Value Decomposition (SVD) to minimise RMSE (Root Mean Square Error) and give great recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our precision function \n",
    "\n",
    "The evaluation metric is Mean Average Precision (MAP) at K (K = 3). MAP is a well-known metric used to evaluate ranked retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=13, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our function to recommend top n challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=3):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 3.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust challenge sequence to ranking by 14-challenge_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = Reader(rating_scale=(1,13))\n",
    "\n",
    "# # Combine training and testing dataset\n",
    "# frames = [df_train, df_test]\n",
    "# df = pd.concat(frames)\n",
    "# ratings = df.copy(deep=True)\n",
    "# ratings['challenge_sequence'] = 14-ratings['challenge_sequence']\n",
    "# ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using n-folds, n = 5, to perform cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_folds = 5\n",
    "# data = Dataset.load_from_df(ratings[['user_id', 'challenge', 'challenge_sequence']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select algorithm for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVDpp()\n",
    "# Run 5-fold cross-validation and print results\n",
    "#cross_validate(algo, data, measures=['FCP', 'RMSE', 'MAE'], cv=n_folds, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD()\n",
    "# Run 5-fold cross-validation and print results\n",
    "#cross_validate(algo, data, measures=['FCP', 'RMSE', 'MAE'], cv=n_folds, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate by AP@K=13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_k = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=n_folds)\n",
    "# algo = SVD()\n",
    "\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     precisions, recalls = precision_recall_at_k(predictions, k=ap_k, threshold=13)\n",
    "\n",
    "#     # Precision and recall can then be averaged over all users\n",
    "#     print('precisions=%f'%(sum(prec for prec in precisions.values()) / len(precisions)))\n",
    "#     print('recalls=%f'%(sum(rec for rec in recalls.values()) / len(recalls)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=n_folds)\n",
    "# algo = SVDpp()\n",
    "\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     precisions, recalls = precision_recall_at_k(predictions, k=ap_k, threshold=4)\n",
    "\n",
    "#     # Precision and recall can then be averaged over all users\n",
    "#     print('precisions=%f'%(sum(prec for prec in precisions.values()) / len(precisions)))\n",
    "#     print('recalls=%f'%(sum(rec for rec in recalls.values()) / len(recalls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to above testing, SVDpp provides better balance on precision and recall\n",
    "\n",
    "Choose SVDpp as major algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVDpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = data.build_full_trainset()\n",
    "# algo.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us pick first user and check the ratings s/he has given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings[ratings['user_id'] == 4576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.predict(4576, 'CI23855', 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = data.build_full_trainset()\n",
    "# algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afile = open(r'./trainset.pkl', 'wb')\n",
    "# pickle.dump(trainset, afile)\n",
    "# afile.close()\n",
    "\n",
    "# afile = open(r'./algo.pkl', 'wb')\n",
    "# pickle.dump(algo, afile)\n",
    "# afile.close()\n",
    "\n",
    "# reload object from file\n",
    "file2 = open(r'./trainset.pkl', 'rb')\n",
    "trainset = pickle.load(file2)\n",
    "file2.close()\n",
    "\n",
    "# file2 = open(r'./algo.pkl', 'rb')\n",
    "# algo = pickle.load(file2)\n",
    "# file2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.predict(4577, 'CI23855', 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "afile = open(r'./testset.pkl', 'wb')\n",
    "pickle.dump(testset, afile)\n",
    "afile.close()\n",
    "\n",
    "#reload object from file\n",
    "# file2 = open(r'./testset.pkl', 'rb')\n",
    "# testset = pickle.load(file2)\n",
    "# file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.set_index(['user_id'])\n",
    "\n",
    "new_test_data = list()\n",
    "# i = 0\n",
    "# for item in testset:\n",
    "#     print('item[0]=%s'%(item[0]))\n",
    "#     i += 1\n",
    "#     if i > 5: continue\n",
    "        \n",
    "for item in testset:\n",
    "    if item[0] in df_test.index:\n",
    "        new_test_data.append(item)\n",
    "    else: pass\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(new_test_data)\n",
    "\n",
    "top_n = get_top_n(predictions, n=3)\n",
    "\n",
    "# Print the recommended items for each user, only get first 7 users as example to view the results.\n",
    "i = 0\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n",
    "    i += 1\n",
    "    if i > 21: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
